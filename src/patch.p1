--- lfadsqueue.py	(original)
+++ lfadsqueue.py	(refactored)
@@ -18,7 +18,7 @@
 import time
 import shlex
 from multiprocessing import Process, Queue, cpu_count, Lock
-from Queue import Empty
+from queue import Empty
 import sys, traceback
 import errno
 
@@ -246,7 +246,7 @@
     try:
         temp = open(outfile, 'a')
     except IOError as exc:
-        print('Error opening out file: {}'.format(exc))
+        print(('Error opening out file: {}'.format(exc)))
 
     temp.close()
 
@@ -258,7 +258,7 @@
     return session in get_list_tmux_sessions()
 
 def get_list_tmux_sessions_name_starts_with(prefix):
-    return filter(lambda sess: sess.startswith(prefix), get_list_tmux_sessions())
+    return [sess for sess in get_list_tmux_sessions() if sess.startswith(prefix)]
 
 class TaskCompletedMessage(object):
     def __init__(self, task_index, success, tail):
@@ -284,7 +284,7 @@
 def process_launch_task_in_tmux(queue, task, gpu_index, filter_output=True):
     """Run command in tmux and monitor output"""
     def print_task(x):
-        print('Task {}: {}'.format(task.name, x.rstrip('\n').strip()))
+        print(('Task {}: {}'.format(task.name, x.rstrip('\n').strip())))
 
     def print_relevant_output_return_success_status(outlines):
         """Prints out relevant lines of output (LFADS specific)
@@ -325,7 +325,7 @@
         # if the tmux session is already running, throw an error, usually means
         # this model is already training
         if check_tmux_session_exists(task.name):
-            print('Queue: Task {} is already running. Monitoring'.format(task.name))
+            print(('Queue: Task {} is already running. Monitoring'.format(task.name)))
             #raise ChildProcessError('Tmux session {} already exists.'.format(task.name))
             pid = None
             task.running_on_gpu = None
@@ -384,8 +384,8 @@
         elif task.is_running():
             num_running += 1
 
-    print('Queue: {0} skipped, {1} finished, {2} failed, {3} running'
-          .format(num_skipped, num_finished, num_failed, num_running))
+    print(('Queue: {0} skipped, {1} finished, {2} failed, {3} running'
+          .format(num_skipped, num_finished, num_failed, num_running)))
 
 def run_command_in_tmux_session_no_monitoring(session_name, command):
     """Run command in tmux session in a detached way, no monitoring of output"""
@@ -396,7 +396,7 @@
     # if the tmux session is already running, throw an error, usually means
     # this model is already training
     if check_tmux_session_exists(session_name):
-        print('Tmux session {} already exists'.format(session_name))
+        print(('Tmux session {} already exists'.format(session_name)))
         raise ChildProcessError('Tmux session {} already exists.'.format(session_name))
 
     with mutex:
@@ -450,7 +450,7 @@
             max_tasks_simultaneously = num_cpus-1
 
     def print_status(x):
-        print('Queue: ' + x.rstrip('\n'))
+        print(('Queue: ' + x.rstrip('\n')))
 
     # is tensorboard running in tmux?
     tensorboard_session_prefix = '{}_tensorboard'.format(queue_name)
@@ -476,7 +476,7 @@
         for task in tasks:
             task.mark_finished_if_donefile_exists()
             if task.skipped_donefile_exists:
-                print('Task {}: skipping, task already completed'.format(task.name))
+                print(('Task {}: skipping, task already completed'.format(task.name)))
 
     # communication queue for each process
     message_queue = Queue(100)
@@ -494,7 +494,7 @@
 
                     if msg.pid is not None:
                         # None means the task was already running previously, so don't print anything
-                        print('Task {}: started in tmux session {} on GPU {} with PID {}'.format(task.name, msg.tmux_session, task.running_on_gpu, msg.pid))
+                        print(('Task {}: started in tmux session {} on GPU {} with PID {}'.format(task.name, msg.tmux_session, task.running_on_gpu, msg.pid)))
 
                         # deduct from gpu memory
                         gpu = find_gpu_by_index(gpu_status, task.running_on_gpu)
@@ -506,14 +506,14 @@
                 elif type(msg) is TaskCompletedMessage:
                     task = tasks[msg.task_index]
                     if msg.success:
-                        print('Task {}: completed successfully'.format(task.name))
+                        print(('Task {}: completed successfully'.format(task.name)))
                     else:
                         task.has_failed = True
                         if len(msg.tail) > 0:
-                            print('Task {}: TERMINATED UNEXPECTEDLY. Final output:'.format(task.name))
-                            print(msg.tail)
+                            print(('Task {}: TERMINATED UNEXPECTEDLY. Final output:'.format(task.name)))
+                            print((msg.tail))
                         else:
-                            print('Task {}: TERMINATED UNEXPECTEDLY with no output'.format(task.name))
+                            print(('Task {}: TERMINATED UNEXPECTEDLY with no output'.format(task.name)))
 
                     task.has_finished = True
                     # return to available gpu memory
@@ -528,8 +528,8 @@
                     task = tasks[msg.task_index]
                     task.has_finished = True
                     task.has_failed = True
-                    print('Task {}: INTERNAL ERROR. Exception was:'.format(task.name))
-                    print(msg.message)
+                    print(('Task {}: INTERNAL ERROR. Exception was:'.format(task.name)))
+                    print((msg.message))
                     do_status_summary = True
 
                     if task.running_on_gpu is not None:
@@ -540,7 +540,7 @@
                     sys.stdout.flush()
 
                 else:
-                    print('Unknown message {}'.format(msg))
+                    print(('Unknown message {}'.format(msg)))
 
             except Empty:
                 pass
